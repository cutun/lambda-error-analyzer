import json
import os
from pathlib import Path
from typing import List, Dict, Any, Optional

import boto3
from botocore.exceptions import BotoCoreError, ClientError

# Resource & Client Initialization
try:
    AWS_REGION = os.environ['AWS_REGION']
    BEDROCK_MODEL_ID = os.environ['BEDROCK_MODEL_ID']
    BEDROCK_RUNTIME = boto3.client(
        service_name="bedrock-runtime",
        region_name=AWS_REGION,
    )
except KeyError as e:
    print(f"FATAL: Missing required environment variable for BedrockSummarizer: {e}")
    BEDROCK_RUNTIME = None
    BEDROCK_MODEL_ID = None


class BedrockSummarizer:
    """
    Uses AWS Bedrock to generate summaries. Placed in a Lambda Layer for reuse.
    """
    def __init__(self):
        if not BEDROCK_RUNTIME or not BEDROCK_MODEL_ID:
            raise EnvironmentError("Bedrock client is not initialized due to missing environment variables.")
            
        self.bedrock_runtime = BEDROCK_RUNTIME
        self.bedrock_model_id = BEDROCK_MODEL_ID

    def _invoke_bedrock(self, prompt: str, system_prompt: str) -> str:
        """Private helper to build the request and invoke the Bedrock model."""
        request_body = self._build_request_body(prompt, system_prompt)
        
        response = self.bedrock_runtime.invoke_model(
            modelId=self.bedrock_model_id,
            body=json.dumps(request_body),
            accept="application/json",
            contentType="application/json",
        )
        response_body = json.loads(response["body"].read())
        
        summary_text = self._extract_text_from_response(response_body)
        if not summary_text:
            raise ValueError(f"Could not find summary text in Bedrock response: {response_body}")
        
        return summary_text.strip()

    def summarize_clusters(self, clusters: List[Dict[str, Any]]) -> str:
        """Generates a summary for a list of log clusters."""
        try:
            # Load the prompts
            prompt_path = Path(__file__).parent / "summarization_prompt.txt" # make sure change this to where your prompt locate to
            prompt_template = prompt_path.read_text()

            log_clusters_text = self._format_clusters_for_prompt(clusters)
            user_prompt = prompt_template.format(log_clusters_text=log_clusters_text)
            system_prompt = "You are an expert systems analyst. Provide a concise, actionable " \
            "summary of these production error clusters."
            return self._invoke_bedrock(user_prompt, system_prompt)
        except FileNotFoundError:
            self.prompt_template = (
                "Error: Prompt file 'summarization_prompt.txt' not found."
            )
        except Exception as e:
            print(f"Bedrock summarization failed: {e}. Falling back to basic summary.")
            return self.generate_fallback_summary(clusters)
            
    def synthesize_summaries(self, summaries: list[str]) -> str:
        """Generates a master summary from a list of other summaries."""
        if not summaries:
            return ""
        try:
            user_prompt = "Individual Summaries:\n" + "\n".join(f"- {s}" for s in summaries)
            system_prompt = (
                "You are an expert systems analyst. The following are several AI-generated summaries "
                "from different batches of a single large log event. Synthesize them into a single, "
                "concise, and actionable master summary. Provide only the final, high-level overview. "
                "Your entire response must be plain text only, without any markdown formatting (no asterisks, underscores, hashes, etc.)."
            )
            return self._invoke_bedrock(user_prompt, system_prompt)
        except Exception as e:
            print(f"Bedrock synthesis failed: {e}. Falling back to joining summaries.")
            return "\n\n---\n\n".join(summaries)

    def _build_request_body(self, user_prompt: str, system_prompt: str) -> Dict[str, Any]:
        """
        Returns the exact JSON payload required by the current model family.
        This allows for easy swapping of model IDs in the environment variables.
        """
        if self.bedrock_model_id.startswith("amazon.nova"):
            # Schema for Amazon Nova models
            return {
                "system": [{"text": system_prompt}],
                "messages": [{"role": "user", "content": [{"text": user_prompt}]}],
                "inferenceConfig": {
                    "maxTokens": 300,
                    "temperature": 0.5,
                    "topP": 0.9,
                },
            }
        else:
            # Default to the schema for Anthropic Claude models
            return {
                "system": system_prompt,
                "messages": [{"role": "user", "content": [{"type": "text", "text": user_prompt}]}],
                "max_tokens": 300,
                "temperature": 0.5,
                "top_p": 0.9,
                "anthropic_version": "bedrock-2023-05-31",
            }
        
    @staticmethod
    def _extract_text_from_response(body: Dict[str, Any]) -> Optional[str]:
        """
        Safely extracts the assistant's reply text from various possible
        Bedrock response structures.
        """
        # Amazon Nova (InvokeModel)
        if "output" in body:
            blocks = (
                body.get("output", {})
                .get("message", {})
                .get("content", [])
            )
            for block in blocks:
                if isinstance(block, dict) and block.get("text"):
                    return block["text"]

        # Claude-family (Anthropic)
        if isinstance(body.get("content"), list):
            first = body["content"][0]
            if isinstance(first, dict) and first.get("text"):
                return first["text"]
            
        # Fallback for older Amazon Titan models
        results_list = body.get("results")
        if isinstance(results_list, list) and results_list:
            first_item = results_list[0]
            if isinstance(first_item, dict):
                return first_item.get("outputText")
        return None

    @staticmethod
    def _format_clusters_for_prompt(clusters: List[Dict[str, Any]]) -> str:
        """Formats clusters as bullet points for a prompt."""
        sorted_clusters = sorted(clusters, key=lambda c: c["count"], reverse=True)
        lines = [f'- Signature: "{c["signature"]}", Occurrences: {c["count"]}' for c in sorted_clusters]
        return "\n".join(lines)

    @staticmethod
    def generate_fallback_summary(clusters: List[Dict[str, Any]]) -> str:
        """Deterministic summary used if the Bedrock API fails."""
        if not clusters: return "No errors detected."
        total_errors = sum(c["count"] for c in clusters)
        num_signatures = len(clusters)
        most_common = clusters[0]
        return (f"AI summary failed. Basic Analysis: Found {total_errors} errors across {num_signatures} unique signatures. The most common error ({most_common['count']} times) was: '{most_common['signature']}'.")

