import json
import re
import hashlib
from datetime import datetime, timezone, timedelta

# --- CONSTANT DEFINITIONS ---
_TS = re.compile(r'^\s*\d{4}-\d{2}-\d{2}[ T]\d{2}:\d{2}:\d{2}(?:[.,]\d+)?Z?\s*')
_TS_TEXT = re.compile(
    r'^\s*(?P<dt>\d{4}-\d{2}-\d{2}[ T]\d{2}:\d{2}:\d{2})' # Main datetime part
    r'(?P<ms>[.,]\d{1,6})?'                             # Optional milliseconds part
)
_BRK = re.compile(r'\[(CRITICAL|FATAL|ERROR|WARNING|INFO|SERVICE|DEBUG|TRACE)\]', re.I)
_BARE = re.compile(r'\b(CRITICAL|FATAL|ERROR|WARNING|INFO|SERVICE|DEBUG|TRACE)\b', re.I)
_SYSLOG_LINE = re.compile(
    r'^(?P<ts>[A-Z][a-z]{2}\s+\d{1,2}\s+\d{2}:\d{2}:\d{2})\s+'
    r'(?P<host>[\w\.-]+)\s+'
    r'(?P<proc>[\w\(\)\d\.-]+(?:\[\d+\])?):\s+'
    r'(?P<msg>.*)$',
    re.I
)

# --- REVISED: More general exception matching ---
# This regex now finds the crucial exception line, whether it's indented or not.
# The brittle _TRACEBACK_IGNORE regex has been removed.
_EXCEPTION_LINE = re.compile(r'^(?:raise\s+)?((?:\w+\.)*\w+(?:Error|Exception))(?:\((.*)\)|:\s*(.*))?.*')

_NORM = [
    (re.compile(r'\b0x[0-9a-fA-F]+\b'), '<hex>'),
    (re.compile(r'\b[0-9a-f]{8}-[0-9a-f]{4}-[1-5][0-9a-f]{3}-[89ab][0-9a-f]{3}-[0-9a-f]{12}\b', re.I), '<uuid>'),
    (re.compile(r'\b\d+\.\d+\.\d+\.\d+\b'), '<ip>'),
    (re.compile(r'\b\d+\b'), '<num>'),
]
_LEVEL_RANK = {'CRITICAL': 5, 'FATAL': 5, 'ERROR': 4, 'WARNING': 3, 'INFO': 2, 'SERVICE': 2, 'DEBUG': 1, 'TRACE': 0}


def _normalise(text: str) -> str:
    for regex, token in _NORM:
        text = regex.sub(token, text)
    return text.strip()

def _first_line(msg: str) -> str:
    if not msg:
        return ""
    # .strip() is important for the patterns that use ^ to anchor to the start.
    return msg.splitlines()[0].strip()

def _hash(text: str) -> str:
    return hashlib.sha1(text.encode()).hexdigest()[:8]

class ExtractSignature:
    def __init__(self, min_severity="WARNING"):
        self.min_level_rank = _LEVEL_RANK.get(min_severity.upper(), 3)
        self.min_severity = min_severity.upper()

    def _extract_timestamp(self, log_message: str, parsed_json: dict = None) -> str:
        # This function remains unchanged
        try:
            if parsed_json:
                ts_val = parsed_json.get('time', parsed_json.get('timestamp', parsed_json.get('ts', parsed_json.get('@timestamp'))))
                if ts_val:
                    if isinstance(ts_val, str) and ts_val.endswith('Z'):
                        ts_val = ts_val[:-1] + '+00:00'
                    return datetime.fromisoformat(ts_val).replace(tzinfo=timezone.utc).isoformat()
            ts_match = _TS_TEXT.search(log_message)
            if ts_match:
                parts = ts_match.groupdict()
                dt_str = parts['dt']
                fmt = '%Y-%m-%dT%H:%M:%S' if 'T' in dt_str else '%Y-%m-%d %H:%M:%S'
                dt_obj = datetime.strptime(dt_str, fmt)
                if parts['ms']:
                    ms_val = parts['ms'].strip('.,')
                    microseconds = int(ms_val.ljust(6, '0'))
                    dt_obj += timedelta(microseconds=microseconds)
                return dt_obj.replace(tzinfo=timezone.utc).isoformat()
        except (ValueError, TypeError):
            pass
        return datetime.now(timezone.utc).isoformat()

    def _parse_syslog_timestamp(self, ts_str: str) -> str:
        # This function remains unchanged
        now = datetime.now(timezone.utc)
        try:
            dt_obj = datetime.strptime(ts_str, '%b %d %H:%M:%S')
            dt_with_year = dt_obj.replace(year=now.year, tzinfo=timezone.utc)
            if dt_with_year > (now + timedelta(days=1)):
                dt_with_year = dt_with_year.replace(year=now.year - 1)
            return dt_with_year.isoformat()
        except ValueError:
            return now.isoformat()

    def extract(self, log_message: str) -> dict | None:
        stripped_line = _first_line(log_message)
        if not stripped_line: return None

        # 1. Handle JSON logs
        if stripped_line.lstrip().startswith('{'):
            # This logic remains the same
            try:
                js = json.loads(stripped_line)
                level_str = str(js.get('level', js.get('severity', 'INFO'))).upper()
                level_rank = _LEVEL_RANK.get(level_str, 2)
                if level_rank < self.min_level_rank: return None
                ts = self._extract_timestamp(stripped_line, parsed_json=js)
                msg = js.get('msg') or js.get('message', '')
                signature = f"{level_str}: {_normalise(msg)}" if msg else f"{level_str}:"
                return {'timestamp': ts, 'level_rank': level_rank, 'signature': signature}
            except json.JSONDecodeError: pass

        # 2. Handle Syslog format
        syslog_match = _SYSLOG_LINE.match(stripped_line)
        if syslog_match:
            # This logic remains the same
            parts = syslog_match.groupdict()
            level_str = 'INFO' 
            level_rank = _LEVEL_RANK[level_str]
            if level_rank < self.min_level_rank: return None
            ts = self._parse_syslog_timestamp(parts['ts'])
            signature = f"{level_str}: {parts['proc']}: {_normalise(parts['msg'])}"
            return {'timestamp': ts, 'level_rank': level_rank, 'signature': signature}

        # 3. Handle standard text logs with explicit severity
        ts = self._extract_timestamp(stripped_line)
        line_no_ts = _TS.sub('', stripped_line).strip()
        m = _BRK.search(line_no_ts) or _BARE.search(line_no_ts)
        if m:
            # This logic remains the same
            level_str = m.group(1).upper()
            level_rank = _LEVEL_RANK.get(level_str, 2)
            if level_rank < self.min_level_rank: return None
            msg_start = m.end()
            candidate = line_no_ts[msg_start:].lstrip(":- ").strip()
            exc_match = re.search(r'\b(\w+(Exception|Error))\b[^:]*:? (.+)', candidate)
            if exc_match:
                signature = f"{level_str}: {_normalise(exc_match.group(1) + ' ' + exc_match.group(3))}"
            else:
                signature = f"{level_str}: {_normalise(candidate)}" if candidate else f"{level_str}"
            return {'timestamp': ts, 'level_rank': level_rank, 'signature': signature}

        # 4. --- NEW GENERIC HANDLING for Exceptions and Indented Lines ---
        
        # First, check if the line is an exception, regardless of indentation
        exc_match = _EXCEPTION_LINE.match(stripped_line)
        if exc_match:
            level_str = "ERROR"
            level_rank = _LEVEL_RANK[level_str]
            if level_rank < self.min_level_rank: return None
            
            exception_type = exc_match.group(1)
            exception_msg = exc_match.group(2) or exc_match.group(3) or ''
            exception_msg = exception_msg.strip().strip("'\"")

            signature = f"ERROR: {_normalise(f'{exception_type}: {exception_msg}')}"
            return {'timestamp': ts, 'level_rank': level_rank, 'signature': signature}

        # If it wasn't an exception, check if the original line was indented.
        # If so, ignore it as it's likely multi-line context.
        original_line = log_message.splitlines()[0]
        if original_line.startswith(' ') or original_line.startswith('\t'):
            return None

        # 5. Fallback for any other unclassified logs
        signature = f"UNCLASSIFIED:{_hash(stripped_line)}"
        return {'timestamp': ts, 'level_rank': 0, 'signature': signature}
    
def parse_signature(signature):
    split_signature = signature.split(":", 1)
    category = split_signature[0].strip().upper()
    message = f"{split_signature[1].strip()}" if len(split_signature)==2 else ""
    for keyword in _LEVEL_RANK.keys():
        if keyword in category:
            level = keyword
            break
    else:
        level = category
    return level, message

