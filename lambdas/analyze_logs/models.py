# lambda_error_analyzer/lambdas/analyze_logs/models.py
import os
from typing import List, Literal
from datetime import datetime, timezone
from pydantic import BaseModel, Field
from pydantic_settings import BaseSettings, SettingsConfigDict

class AppSettings(BaseSettings):
    """
    Loads environment variables for the application.
    """
    model_config = SettingsConfigDict(
        env_file='.env',
        env_file_encoding='utf-8',
        extra='ignore',
    )

    # AWS specific settings
    aws_region: str = Field("us-east-1", alias='AWS_REGION')
    log_bucket: str = Field("dummy-log-bucket", alias='LOG_BUCKET')
    dynamodb_table_name: str = Field("LogAnalysisResults", alias='DYNAMODB_TABLE_NAME')
    error_state_table_name: str = Field("LogErrorStates", alias='ERROR_STATE_TABLE_NAME')
    sns_topic_arn: str = Field("arn:aws:sns:us-east-1:123456789012:DummyTopic", alias='SNS_TOPIC_ARN')
    
    # Bedrock specific settings
    bedrock_model_id: str = Field("amazon.nova-micro-v1:0", alias='BEDROCK_MODEL_ID')

    # Recurrence and Anomaly detection thresholds
    recurrence_count_threshold: int = Field(5, alias='RECURRENCE_COUNT_THRESHOLD')
    recurrence_time_window_seconds: int = Field(3600, alias='RECURRENCE_TIME_WINDOW_SECONDS') # 1 hour
    # --- NEW: Baseline rate for calculating anomaly score on the first day ---
    default_baseline_rate_per_hour: float = Field(0.1, alias='DEFAULT_BASELINE_RATE_PER_HOUR')


# Helper function to instantiate settings
def get_settings() -> AppSettings:
    return AppSettings()

class LogCluster(BaseModel):
    """
    Represents a cluster of similar log entries.
    """
    signature: str = Field(..., description="A unique signature representing the error type.")
    count: int = Field(..., description="The number of times this error signature occurred in the current batch.")
    log_samples: List[str] = Field(..., description="A list of raw log messages in the cluster.")
    representative_log: str = Field(..., description="An example log from the cluster, useful for summaries.")
    is_recurring: bool = Field(False, description="True if this error signature has breached recurrence thresholds.")
    # --- NEW: Anomaly score for each cluster ---
    anomaly_score: float = Field(0.0, description="A score representing how unusual the current error rate is compared to its historical baseline.")


class LogAnalysisResult(BaseModel):
    """
    Defines the final JSON structure for the analysis output.
    """
    analysis_id: str = Field(..., description="A unique identifier for this analysis batch (Partition Key).")
    summary: str = Field(..., description="The natural-language summary generated by Bedrock.")
    total_logs_processed: int = Field(..., description="Total number of raw log entries processed.")
    total_clusters_found: int = Field(..., description="Total number of distinct error clusters found.")
    clusters: List[LogCluster] = Field(..., description="A list of all log clusters identified.")
    processed_at: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))
    gsi1pk: Literal["ANALYSIS_RESULT"] = "ANALYSIS_RESULT"
