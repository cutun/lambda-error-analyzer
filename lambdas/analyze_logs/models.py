# lambda_error_analyzer/lambdas/analyze_logs/models.py
import os
from typing import List, Literal # --- NEW: Import Literal ---
from datetime import datetime, timezone
from pydantic import BaseModel, Field
from pydantic_settings import BaseSettings, SettingsConfigDict

class AppSettings(BaseSettings):
    """
    Loads environment variables for the application.
    These settings are now aligned with AWS services.
    """
    model_config = SettingsConfigDict(
        env_file='.env',
        env_file_encoding='utf-8',
        extra='ignore',
    )

    # AWS specific settings
    aws_region: str = Field("us-east-1", alias='AWS_REGION')
    log_bucket: str = Field("dummy-log-bucket", alias='LOG_BUCKET')
    dynamodb_table_name: str = Field("dummy-log-clusters-table", alias='DYNAMODB_TABLE_NAME')
    sns_topic_arn: str = Field("arn:aws:sns:us-east-1:123456789012:DummyTopic", alias='SNS_TOPIC_ARN')
    
    # Bedrock specific settings
    bedrock_model_id: str = Field("amazon.nova-micro-v1:0", alias='BEDROCK_MODEL_ID')

# Helper function to instantiate settings within Lambda handlers.
def get_settings() -> AppSettings:
    return AppSettings()

class LogCluster(BaseModel):
    """
    Represents a cluster of similar log entries.
    """
    signature: str = Field(..., description="A unique signature representing the error type.")
    count: int = Field(..., description="The number of times this error signature occurred.")
    log_samples: List[str] = Field(..., description="A list of raw log messages in the cluster.")
    representative_log: str = Field(..., description="An example log from the cluster, useful for summaries.")

class LogAnalysisResult(BaseModel):
    """
    Defines the final JSON structure for the analysis output.
    This object will be stored in DynamoDB or sent via SNS.
    """
    analysis_id: str = Field(..., description="A unique identifier for this analysis batch (Partition Key).")
    summary: str = Field(..., description="The natural-language summary generated by Bedrock.")
    total_logs_processed: int = Field(..., description="Total number of raw log entries processed.")
    total_clusters_found: int = Field(..., description="Total number of distinct error clusters found.")
    clusters: List[LogCluster] = Field(..., description="A list of all log clusters identified.")

    # --- Fields for sorting and indexing ---
    processed_at: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))
    # --- FIX: Changed from 'const=True' to use Literal for Pydantic v2 compatibility ---
    gsi1pk: Literal["ANALYSIS_RESULT"] = "ANALYSIS_RESULT"